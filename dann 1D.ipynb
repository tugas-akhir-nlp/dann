{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import keras.layers as kl\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "K.set_image_data_format('channels_first')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from Gradient_Reverse_Layer import GradientReversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import gzip\n",
    "\n",
    "\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.preprocessing import sequence\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(object):\n",
    "    def __init__(self, width=28, channels = 1, classes=1, features=1, batch_size=1, grl='auto', summary=False, model_plot=False):\n",
    "        ## Set Defualts\n",
    "        self.learning_phase = K.variable(1)\n",
    "        self.domain_invariant_features = None\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "        self.input_shape = (width)\n",
    "        self.classes = classes\n",
    "        self.features = features\n",
    "        self.batch_size = batch_size\n",
    "        self.grl = 'auto'\n",
    "        # Set reversal gradient value.\n",
    "        if grl is 'auto':\n",
    "            self.grl_rate = 1.0\n",
    "        else:\n",
    "            self.grl_rate = grl\n",
    "        self.summary = summary\n",
    "        self.model_plot = model_plot\n",
    "\n",
    "        # Build the model\n",
    "        self.model = self._build()\n",
    "        \n",
    "        # Print and Save the model summary if requested.\n",
    "        if self.summary:\n",
    "            self.model.summary()\n",
    "        if self.model_plot:\n",
    "            plot_model(self.model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    def feature_extractor(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the feature extractor part.\n",
    "        '''\n",
    "#         out = kl.Conv2D(filters=32, kernel_size=(5, 5), padding=\"same\",\n",
    "#                         activation=\"relu\", data_format='channels_first')(inp)\n",
    "#         out = kl.MaxPooling2D(pool_size=(2, 2))(out)\n",
    "        \n",
    "        nb_filter = 100\n",
    "        filter_length = 3\n",
    "        out = Convolution1D(filters=nb_filter,\n",
    "                       kernel_size=filter_length,\n",
    "                       padding='same',\n",
    "                       activation='relu',\n",
    "                       strides=1)(inp)\n",
    "        out = GlobalMaxPooling1D()(out)\n",
    "\n",
    "\n",
    "        out = kl.Dropout(0.5)(out)\n",
    "        \n",
    "\n",
    "        feature_output = kl.Dense(self.features, activation=\"relu\")(out)\n",
    "        self.domain_invariant_features = feature_output\n",
    "        return feature_output\n",
    "\n",
    "    def classifier(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the classifier part.\n",
    "        '''\n",
    "#         out = kl.Dense(128, activation=\"relu\")(inp)\n",
    "#         out = kl.Dropout(0.5)(out)\n",
    "        classifier_output = kl.Dense(self.classes, activation=\"softmax\", name=\"classifier_output\")(inp)\n",
    "        return classifier_output\n",
    "\n",
    "    def discriminator(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the discriminator part.\n",
    "        '''\n",
    "        out = kl.Dense(128, activation=\"relu\")(inp)\n",
    "#         out = kl.Dropout(0.5)(out)\n",
    "        discriminator_output = kl.Dense(2, activation=\"softmax\", name=\"discriminator_output\")(out)\n",
    "        return discriminator_output\n",
    "\n",
    "    def _build(self):\n",
    "        '''\n",
    "        This function builds the network based on the Feature Extractor, Classifier and Discriminator parts.\n",
    "        '''\n",
    "#        BEGIN - UBAH INPUT\n",
    "\n",
    "        words_input = Input(shape=(self.width,), dtype='int32', name='words_input')\n",
    "        words = Embedding(embeddings.shape[0], embeddings.shape[1], weights=[embeddings], trainable=False)(words_input)\n",
    "\n",
    "        distance1_input = Input(shape=(self.width,), dtype='int32', name='distance1_input')\n",
    "        distance1 = Embedding(max_position, position_dims)(distance1_input)\n",
    "\n",
    "        distance2_input = Input(shape=(self.width,), dtype='int32', name='distance2_input')\n",
    "        distance2 = Embedding(max_position, position_dims)(distance2_input)\n",
    "        print('77777777777777777777777')\n",
    "        print(words_input.shape)\n",
    "        \n",
    "#         CUBO RESHAPE, GAGAL KARENA INPUT LAYER BERUBAH\n",
    "#         s = tf.shape(words_input)\n",
    "#         print(s)\n",
    "#         words_input = tf.reshape(words_input, [-1, self.input_shape[0], self.input_shape[1], 300])\n",
    "#         print(words_input.shape)\n",
    "# #         s = tf.shape(a)\n",
    "#         c = tf.reshape(b, shape=[s[0], s[1], 3])\n",
    "#         words_input = np.expand_dims(words_input, axis=3)\n",
    "#         words_input = np.tile(trainX, [1, 1, 1, 300])\n",
    "        \n",
    "#         distance1_input = np.expand_dims(distance1_input, axis=3)\n",
    "#         distance1_input = np.tile(trainX, [1, 1, 1, 50])\n",
    "        \n",
    "#         distance2_input = np.expand_dims(distance2_input, axis=3)\n",
    "#         distance2_input = np.tile(trainX, [1, 1, 1, 50])\n",
    "        \n",
    "\n",
    "        inp = concatenate([words, distance1, distance2])\n",
    "        inp = [words, distance1, distance2]\n",
    "        input_array = [words_input, distance1_input, distance2_input]\n",
    "\n",
    "#        END UBAH INPUT\n",
    "\n",
    "        import time\n",
    "        print('000000000000000000000000000')\n",
    "        print(inp[0].shape)\n",
    "#         BEGIN CNN\n",
    "        output = concatenate([words, distance1, distance2])\n",
    "        \n",
    "#         output = tf.reshape(output, [-1, self.input_shape[0], self.input_shape[1], 400])\n",
    "#         print(output.shape)\n",
    "        \n",
    "        feature_output = self.feature_extractor(output)\n",
    "#         END CNN\n",
    "        \n",
    "        self.grl_layer = GradientReversal(1.0)\n",
    "        feature_output_grl = self.grl_layer(feature_output)\n",
    "        labeled_feature_output = kl.Lambda(lambda x: K.switch(K.learning_phase(),\n",
    "                                                              K.concatenate([x[:int(self.batch_size//2)],\n",
    "                                                                             x[:int(self.batch_size//2)]], axis=0), x),\n",
    "                                           output_shape=lambda x: x[0:])(feature_output_grl)\n",
    "\n",
    "        classifier_output = self.classifier(labeled_feature_output)\n",
    "        discriminator_output = self.discriminator(feature_output)\n",
    "        \n",
    "        print('111111111111111111')\n",
    "        print(input_array[0].shape)\n",
    "        model = keras.models.Model(inputs=input_array, outputs=[discriminator_output, classifier_output])\n",
    "        return model\n",
    "\n",
    "    def batch_generator(self, trainX, trainY=None, batch_size=1, shuffle=True):\n",
    "        '''\n",
    "        This function generates batches for the training purposes.\n",
    "        '''\n",
    "        print('trainX.shape[0]')\n",
    "        print(trainX.shape[0])\n",
    "        print('44444444444444444444444')\n",
    "        print('batch_size, self.channels, self.width, self.height')\n",
    "        print(batch_size, self.channels, self.width)\n",
    "\n",
    "\n",
    "        if shuffle:\n",
    "            index = np.random.randint(0, len(trainX) - batch_size)\n",
    "        else:\n",
    "            index = np.arange(0, len(trainX), batch_size)\n",
    "        while trainX.shape[0] > index + batch_size:\n",
    "            batch = trainX[index : index + batch_size]\n",
    "            batch = batch.reshape(batch_size, self.width, self.channels*2)\n",
    "            if trainY is not None:\n",
    "                batch_labels = trainY[index : index + batch_size]\n",
    "                yield batch, batch_labels\n",
    "            else:\n",
    "                yield batch\n",
    "            index += batch_size\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        '''\n",
    "        This function compiles the model based on the given optimization method and its parameters.\n",
    "        '''\n",
    "        self.model.compile(optimizer=optimizer, loss={'classifier_output': 'binary_crossentropy', 'discriminator_output': 'binary_crossentropy'}, loss_weights={'classifier_output': 0.5, 'discriminator_output': 1.0})\n",
    "\n",
    "    def train(self, trainX, trainDX, trainY=None, epochs=1, batch_size=1, verbose=True, save_model=None):\n",
    "        '''\n",
    "        This function trains the model using the input and target data, and saves the model if specified.\n",
    "        '''\n",
    "        print('222222222222222')\n",
    "        print(trainX.shape)\n",
    "        # Prepare batch \n",
    "        for cnt in range(epochs):\n",
    "#             data for the model training.\n",
    "            Labeled = self.batch_generator(trainX, trainY, batch_size=batch_size // 2)\n",
    "            UNLabeled = self.batch_generator(trainDX, batch_size=batch_size // 2)\n",
    "            \n",
    "            # Settings for learning rate.\n",
    "            p = np.float(cnt) / epochs\n",
    "            lr = 0.01 / (1. + 10 * p)**0.75\n",
    "\n",
    "            # Settings for reverse gradient magnitude (if it's set to be automatically calculated, otherwise set by user.)\n",
    "            if self.grl is 'auto':\n",
    "                self.grl_layer.l = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "\n",
    "            # Re-compile model to adopt new learning rate and gradient reversal value.\n",
    "            self.compile(keras.optimizers.SGD(lr))\n",
    "            \n",
    "            # Loop over each batch and train the model.\n",
    "            for batchX, batchY in Labeled:\n",
    "                # Get the batch for unlabeled data. If the batches are finished, regenerate the batches agian.\n",
    "                try:\n",
    "                    batchDX = next(UNLabeled)\n",
    "                except:\n",
    "                    UNLabeled = self.batch_generator(trainDX, batch_size=batch_size // 2)\n",
    "                # Combine the labeled and unlabeled images along with the discriminative results.\n",
    "                combined_batchX = np.concatenate((batchX, batchDX))\n",
    "                batch2Y = np.concatenate((batchY, batchY))\n",
    "                combined_batchY = np.concatenate((np.tile([0, 1], [batchX.shape[0], 1]), np.tile([1, 0], [batchDX.shape[0], 1])))\n",
    "                # Train the model\n",
    "                \n",
    "                metrics = self.model.train_on_batch({'words_input': combined_batchX,\n",
    "                                                    'distance1_input': combined_batchX,\n",
    "                                                    'distance2_input': combined_batchX},\n",
    "                                                    {'classifier_output': batch2Y, 'discriminator_output':combined_batchY})\n",
    "            # Print the losses if asked for.\n",
    "            if verbose:\n",
    "                print(\"Epoch {}/{}\\n\\t[Generator_loss: {:.4}, Discriminator_loss: {:.4}, Classifier_loss: {:.4}]\".format(cnt+1, epochs, metrics[0], metrics[1], metrics[2]))\n",
    "        # Save the model if asked for.\n",
    "        if save_model is not None and isinstance(save_model, str):\n",
    "            if save_model[-3:] is not \".h5\":\n",
    "                save_model = ''.join((save_model, \".h5\"))\n",
    "            self.model.save(save_model)\n",
    "        elif save_model is not None and not isinstance(save_model, str):\n",
    "            raise TypeError(\"The input must be a filename for model settings in string format.\")\n",
    "\n",
    "\n",
    "    def evaluate(self, testX, testY=None, weight_loc=None, save_pred=None, verbose=False):\n",
    "        '''\n",
    "        This function evaluates the model, and generates the predicted classes.\n",
    "        '''\n",
    "        if weight_loc is not None:\n",
    "            self.compile(keras.optimizers.SGD())\n",
    "            self.model.load_weights(weight_loc)\n",
    "        _, yhat_class = self.model.predict(testX, verbose=verbose)\n",
    "        if save_pred is not None:\n",
    "            np.save(save_pred, yhat_class)\n",
    "        if testY is not None and len(testY) == 2:\n",
    "            acc = self.model.evaluate(testX, testY, verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"The classifier and discriminator metrics for evaluation are [{}, {}]\".format(acc[0], acc[1]))\n",
    "        elif testY is not None and len(testY) == 1:\n",
    "            acc = self.model.evaluate(testX, [np.ones((testY.shape[0], 2)), testY], verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"The classifier metric for evaluation is {}\".format(acc[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset\n",
      "sentenceTrain :  (8000, 97)\n",
      "positionTrain1 :  (8000, 97)\n",
      "positionTrain2 :  (8000, 97)\n",
      "sentenceTest :  (2000, 97)\n",
      "positionTest1 :  (2000, 97)\n",
      "positionTest2 :  (2000, 97)\n",
      "333333333333333\n",
      "(8000, 97, 2)\n",
      "(8000, 1, 97, 2)\n",
      "77777777777777777777777\n",
      "(?, 97)\n",
      "000000000000000000000000000\n",
      "(?, 97, 300)\n",
      "111111111111111111\n",
      "(?, 97)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "words_input (InputLayer)        (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distance1_input (InputLayer)    (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distance2_input (InputLayer)    (None, 97)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_184 (Embedding)       (None, 97, 300)      6158100     words_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_185 (Embedding)       (None, 97, 50)       3200        distance1_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_186 (Embedding)       (None, 97, 50)       3200        distance2_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 97, 400)      0           embedding_184[0][0]              \n",
      "                                                                 embedding_185[0][0]              \n",
      "                                                                 embedding_186[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 97, 100)      120100      concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 100)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 100)          0           global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_79 (Dense)                (None, 32)           3232        dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gradient_reversal_40 (GradientR (None, 32)           0           dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_80 (Dense)                (None, 128)          4224        dense_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 32)           0           gradient_reversal_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "discriminator_output (Dense)    (None, 2)            258         dense_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "classifier_output (Dense)       (None, 10)           330         lambda_40[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,292,644\n",
      "Trainable params: 134,544\n",
      "Non-trainable params: 6,158,100\n",
      "__________________________________________________________________________________________________\n",
      "222222222222222\n",
      "(8000, 1, 97, 2)\n",
      "trainX.shape[0]\n",
      "8000\n",
      "44444444444444444444444\n",
      "batch_size, self.channels, self.width, self.height\n",
      "16 1 97\n",
      "trainX.shape[0]\n",
      "6000\n",
      "44444444444444444444444\n",
      "batch_size, self.channels, self.width, self.height\n",
      "16 1 97\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected words_input to have 2 dimensions, but got array with shape (32, 97, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-449ede3e2f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mdann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDANN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sentence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mdann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;31m# Evaluate for binary MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mdann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./binary_testX.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-36e807f8a7bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainX, trainDX, trainY, epochs, batch_size, verbose, save_model)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                                     \u001b[0;34m'distance1_input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcombined_batchX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                                                     'distance2_input': combined_batchX},\n\u001b[0;32m--> 210\u001b[0;31m                                                     {'classifier_output': batch2Y, 'discriminator_output':combined_batchY})\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;31m# Print the losses if asked for.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensor-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensor-gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/tensor-gpu/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected words_input to have 2 dimensions, but got array with shape (32, 97, 2)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "# LOAD DATASET\n",
    "print(\"Load dataset\")\n",
    "f = gzip.open('pkl/sem-relations.pkl.gz', 'rb')\n",
    "data = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "position_dims = 50\n",
    "\n",
    "embeddings = data['wordEmbeddings']\n",
    "# naming rule \n",
    "yTrain, sentenceTrain, position1Train, position2Train = data['train_set']\n",
    "yTest, sentenceTest, position1Test, position2Test = data['test_set']\n",
    "XTrain = [sentenceTrain, position1Train, position2Train]\n",
    "XTest  = [sentenceTest, position1Test, position2Test]\n",
    "\n",
    "# out of domain data, temporary use same data as above\n",
    "temp, sentenceTrainDX, position1TrainDX, position2TrainDX = data['train_set']\n",
    "temp, sentenceTestDX, position1TestDX, position2TestDX = data['test_set']\n",
    "\n",
    "DXTrain = [sentenceTrainDX, position1TrainDX, position2TrainDX]\n",
    "DXTest = [sentenceTestDX, position1TestDX, position2TestDX]\n",
    "\n",
    "max_position = max(np.max(position1Train), np.max(position2Train)) + 1\n",
    "\n",
    "n_out = max(yTrain) + 1\n",
    "\n",
    "max_sentence_len = sentenceTrain.shape[1]\n",
    "\n",
    "print(\"sentenceTrain : \", sentenceTrain.shape)\n",
    "print(\"positionTrain1 : \", position1Train.shape)\n",
    "print(\"positionTrain2 : \", position2Train.shape)\n",
    "\n",
    "print(\"sentenceTest : \", sentenceTest.shape)\n",
    "print(\"positionTest1 : \", position1Test.shape)\n",
    "print(\"positionTest2 : \", position2Test.shape)\n",
    "\n",
    "\n",
    "# Process data to match the model specs.\n",
    "print(\"333333333333333\")\n",
    "trainX = np.expand_dims(sentenceTrain, axis=2)\n",
    "trainX = np.tile(trainX, [1, 1, 2]).transpose(0, 1, 2)\n",
    "trainY = yTrain\n",
    "\n",
    "print(trainX.shape)\n",
    "\n",
    "trainX = np.expand_dims(trainX, axis=3)\n",
    "trainX = np.tile(trainX, [1, 1, 1, 1]).transpose(0, 3, 1, 2)\n",
    "print(trainX.shape)\n",
    "\n",
    "# # Divide train and test sets (the only limitation of this model is to assign even number as batch_sizes and the size of data should yield 0 remainder.)\n",
    "# trainX, testX = trainX[:45000], trainX[45000:55016]\n",
    "# trainY = keras.utils.to_categorical(trainY, num_classes=10)\n",
    "# trainY, testY = trainY[:45000], trainY[45000:55016]\n",
    "\n",
    "# Load MNIST-M dataset as out-of-domain and unlabeled data     \n",
    "trainDX = trainX[:6000]\n",
    "testDX = trainX[6000:8000]\n",
    "mnist_m = None\n",
    "\n",
    "# # Rescale -1 to 1\n",
    "# trainX = (trainX.astype(np.float32) - 127.5) / 127.5\n",
    "# trainDX = (trainDX.astype(np.float32) - 127.5) / 127.5\n",
    "# testX = (testX.astype(np.float32) - 127.5) / 127.5\n",
    "# testDX = (testDX.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Initiate the model\n",
    "dann = DANN(summary=True, width=max_sentence_len, channels=1, classes=10, features=32, batch_size=32, model_plot=True)\n",
    "# Train the model\n",
    "dann.train(trainX, trainDX, trainY, epochs=100, batch_size=32)\n",
    "# Evaluate for binary MNIST\n",
    "dann.evaluate(testX, testY, save_pred=\"./binary_testX.npy\", verbose=True)\n",
    "# Evaluate for colorful MNIST (MNIST-M)\n",
    "dann.evaluate(testDX, save_pred=\"./colorful_testX.npy\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
