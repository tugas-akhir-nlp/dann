{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import keras.layers as kl\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "K.set_image_data_format('channels_first')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from Gradient_Reverse_Layer import GradientReversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "import gzip\n",
    "\n",
    "\n",
    "import sys\n",
    "if (sys.version_info > (3, 0)):\n",
    "    import pickle as pkl\n",
    "else: #Python 2.7 imports\n",
    "    import cPickle as pkl\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.regularizers import Regularizer\n",
    "from keras.preprocessing import sequence\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANN(object):\n",
    "    def __init__(self, width=28, height = 28, channels = 1, classes=1, features=1, batch_size=1, grl='auto', summary=False, model_plot=False):\n",
    "        ## Set Defualts\n",
    "        self.learning_phase = K.variable(1)\n",
    "        self.domain_invariant_features = None\n",
    "        self.width = width\n",
    "        self.height = 2\n",
    "        self.channels = channels\n",
    "        self.input_shape = (channels, width)\n",
    "        self.classes = classes\n",
    "        self.features = features\n",
    "        self.batch_size = batch_size\n",
    "        self.grl = 'auto'\n",
    "        # Set reversal gradient value.\n",
    "        if grl is 'auto':\n",
    "            self.grl_rate = 1.0\n",
    "        else:\n",
    "            self.grl_rate = grl\n",
    "        self.summary = summary\n",
    "        self.model_plot = model_plot\n",
    "\n",
    "        # Build the model\n",
    "        self.model = self._build()\n",
    "        \n",
    "        # Print and Save the model summary if requested.\n",
    "        if self.summary:\n",
    "            self.model.summary()\n",
    "        if self.model_plot:\n",
    "            plot_model(self.model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    def feature_extractor(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the feature extractor part.\n",
    "        '''\n",
    "        out = kl.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "                        activation=\"relu\", data_format='channels_first')(inp)\n",
    "        out = kl.MaxPooling2D(pool_size=(2, 2))(out)\n",
    "        \n",
    "        nb_filter = 100\n",
    "        filter_length = 3\n",
    "#         out = Convolution1D(filters=nb_filter,\n",
    "#                        kernel_size=filter_length,\n",
    "#                        padding='same',\n",
    "#                        activation='relu',\n",
    "#                        strides=1)(inp)\n",
    "#         out = GlobalMaxPooling1D()(out)\n",
    "\n",
    "\n",
    "        out = kl.Dropout(0.5)(out)\n",
    "#         out = kl.Flatten()(out)\n",
    "\n",
    "\n",
    "        feature_output = kl.Dense(self.features, activation=\"relu\")(out)\n",
    "        self.domain_invariant_features = feature_output\n",
    "        return feature_output\n",
    "\n",
    "    def classifier(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the classifier part.\n",
    "        '''\n",
    "#         out = kl.Dense(128, activation=\"relu\")(inp)\n",
    "#         out = kl.Dropout(0.5)(out)\n",
    "        classifier_output = kl.Dense(self.classes, activation=\"softmax\", name=\"classifier_output\")(inp)\n",
    "        return classifier_output\n",
    "\n",
    "    def discriminator(self, inp):\n",
    "        ''' \n",
    "        This function defines the structure of the discriminator part.\n",
    "        '''\n",
    "        out = kl.Dense(128, activation=\"relu\")(inp)\n",
    "#         out = kl.Dropout(0.5)(out)\n",
    "        discriminator_output = kl.Dense(2, activation=\"softmax\", name=\"discriminator_output\")(out)\n",
    "        return discriminator_output\n",
    "\n",
    "    def _build(self):\n",
    "        '''\n",
    "        This function builds the network based on the Feature Extractor, Classifier and Discriminator parts.\n",
    "        '''\n",
    "#        BEGIN - UBAH INPUT\n",
    "\n",
    "        words_input = Input(shape=self.input_shape, dtype='int32', name='words_input')\n",
    "        words = Embedding(embeddings.shape[0], embeddings.shape[1], weights=[embeddings], trainable=False)(words_input)\n",
    "\n",
    "        distance1_input = Input(shape=self.input_shape, dtype='int32', name='distance1_input')\n",
    "        distance1 = Embedding(max_position, position_dims)(distance1_input)\n",
    "\n",
    "        distance2_input = Input(shape=self.input_shape, dtype='int32', name='distance2_input')\n",
    "        distance2 = Embedding(max_position, position_dims)(distance2_input)\n",
    "        print('77777777777777777777777')\n",
    "        print(words_input.shape)\n",
    "        \n",
    "#         CUBO RESHAPE, GAGAL KARENA INPUT LAYER BERUBAH\n",
    "#         s = tf.shape(words_input)\n",
    "#         print(s)\n",
    "#         words_input = tf.reshape(words_input, [-1, self.input_shape[0], self.input_shape[1], 300])\n",
    "#         print(words_input.shape)\n",
    "# #         s = tf.shape(a)\n",
    "#         c = tf.reshape(b, shape=[s[0], s[1], 3])\n",
    "#         words_input = np.expand_dims(words_input, axis=3)\n",
    "#         words_input = np.tile(trainX, [1, 1, 1, 300])\n",
    "        \n",
    "#         distance1_input = np.expand_dims(distance1_input, axis=3)\n",
    "#         distance1_input = np.tile(trainX, [1, 1, 1, 50])\n",
    "        \n",
    "#         distance2_input = np.expand_dims(distance2_input, axis=3)\n",
    "#         distance2_input = np.tile(trainX, [1, 1, 1, 50])\n",
    "        \n",
    "\n",
    "        inp = concatenate([words, distance1, distance2])\n",
    "        inp = [words, distance1, distance2]\n",
    "        input_array = [words_input, distance1_input, distance2_input]\n",
    "\n",
    "#        END UBAH INPUT\n",
    "\n",
    "        import time\n",
    "        print('000000000000000000000000000')\n",
    "        print(inp[0].shape)\n",
    "#         BEGIN CNN\n",
    "        output = concatenate([words, distance1, distance2])\n",
    "        \n",
    "#         output = tf.reshape(output, [-1, self.input_shape[0], self.input_shape[1], 400])\n",
    "#         print(output.shape)\n",
    "        \n",
    "        feature_output = self.feature_extractor(output)\n",
    "#         END CNN\n",
    "        \n",
    "        self.grl_layer = GradientReversal(1.0)\n",
    "        feature_output_grl = self.grl_layer(feature_output)\n",
    "        labeled_feature_output = kl.Lambda(lambda x: K.switch(K.learning_phase(), K.concatenate([x[:int(self.batch_size//2)], x[:int(self.batch_size//2)]], axis=0), x), output_shape=lambda x: x[0:])(feature_output_grl)\n",
    "\n",
    "        classifier_output = self.classifier(labeled_feature_output)\n",
    "        discriminator_output = self.discriminator(feature_output)\n",
    "        \n",
    "        print('111111111111111111')\n",
    "        print(input_array[0].shape)\n",
    "        model = keras.models.Model(inputs=input_array, outputs=[discriminator_output, classifier_output])\n",
    "        return model\n",
    "\n",
    "    def batch_generator(self, trainX, trainY=None, batch_size=1, shuffle=True):\n",
    "        '''\n",
    "        This function generates batches for the training purposes.\n",
    "        '''\n",
    "        print('trainX.shape[0]')\n",
    "        print(trainX.shape[0])\n",
    "        print('44444444444444444444444')\n",
    "        print('batch_size, self.channels, self.width, self.height')\n",
    "        print(batch_size, self.channels, self.width, self.height)\n",
    "\n",
    "\n",
    "        if shuffle:\n",
    "            index = np.random.randint(0, len(trainX) - batch_size)\n",
    "        else:\n",
    "            index = np.arange(0, len(trainX), batch_size)\n",
    "        while trainX.shape[0] > index + batch_size:\n",
    "            batch = trainX[index : index + batch_size]\n",
    "            batch = batch.reshape(batch_size, self.width, self.height, self.channels)\n",
    "            if trainY is not None:\n",
    "                batch_labels = trainY[index : index + batch_size]\n",
    "                yield batch, batch_labels\n",
    "            else:\n",
    "                yield batch\n",
    "            index += batch_size\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        '''\n",
    "        This function compiles the model based on the given optimization method and its parameters.\n",
    "        '''\n",
    "        self.model.compile(optimizer=optimizer, loss={'classifier_output': 'binary_crossentropy', 'discriminator_output': 'binary_crossentropy'}, loss_weights={'classifier_output': 0.5, 'discriminator_output': 1.0})\n",
    "\n",
    "    def train(self, trainX, trainDX, trainY=None, epochs=1, batch_size=1, verbose=True, save_model=None):\n",
    "        '''\n",
    "        This function trains the model using the input and target data, and saves the model if specified.\n",
    "        '''\n",
    "        print('222222222222222')\n",
    "        print(trainX.shape)\n",
    "        # Prepare batch \n",
    "        for cnt in range(epochs):\n",
    "#             data for the model training.\n",
    "            Labeled = self.batch_generator(trainX, trainY, batch_size=batch_size // 2)\n",
    "            UNLabeled = self.batch_generator(trainDX, batch_size=batch_size // 2)\n",
    "            \n",
    "            # Settings for learning rate.\n",
    "            p = np.float(cnt) / epochs\n",
    "            lr = 0.01 / (1. + 10 * p)**0.75\n",
    "\n",
    "            # Settings for reverse gradient magnitude (if it's set to be automatically calculated, otherwise set by user.)\n",
    "            if self.grl is 'auto':\n",
    "                self.grl_layer.l = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "\n",
    "            # Re-compile model to adopt new learning rate and gradient reversal value.\n",
    "            self.compile(keras.optimizers.SGD(lr))\n",
    "            \n",
    "            # Loop over each batch and train the model.\n",
    "            for batchX, batchY in Labeled:\n",
    "                # Get the batch for unlabeled data. If the batches are finished, regenerate the batches agian.\n",
    "                try:\n",
    "                    batchDX = next(UNLabeled)\n",
    "                except:\n",
    "                    UNLabeled = self.batch_generator(trainDX, batch_size=batch_size // 2)\n",
    "                # Combine the labeled and unlabeled images along with the discriminative results.\n",
    "                combined_batchX = np.concatenate((batchX, batchDX))\n",
    "                batch2Y = np.concatenate((batchY, batchY))\n",
    "                combined_batchY = np.concatenate((np.tile([0, 1], [batchX.shape[0], 1]), np.tile([1, 0], [batchDX.shape[0], 1])))\n",
    "                # Train the model\n",
    "                \n",
    "                metrics = self.model.train_on_batch({'words_input': combined_batchX,\n",
    "                                                    'distance1_input': combined_batchX,\n",
    "                                                    'distance2_input': combined_batchX},\n",
    "                                                    {'classifier_output': batch2Y, 'discriminator_output':combined_batchY})\n",
    "            # Print the losses if asked for.\n",
    "            if verbose:\n",
    "                print(\"Epoch {}/{}\\n\\t[Generator_loss: {:.4}, Discriminator_loss: {:.4}, Classifier_loss: {:.4}]\".format(cnt+1, epochs, metrics[0], metrics[1], metrics[2]))\n",
    "        # Save the model if asked for.\n",
    "        if save_model is not None and isinstance(save_model, str):\n",
    "            if save_model[-3:] is not \".h5\":\n",
    "                save_model = ''.join((save_model, \".h5\"))\n",
    "            self.model.save(save_model)\n",
    "        elif save_model is not None and not isinstance(save_model, str):\n",
    "            raise TypeError(\"The input must be a filename for model settings in string format.\")\n",
    "\n",
    "\n",
    "    def evaluate(self, testX, testY=None, weight_loc=None, save_pred=None, verbose=False):\n",
    "        '''\n",
    "        This function evaluates the model, and generates the predicted classes.\n",
    "        '''\n",
    "        if weight_loc is not None:\n",
    "            self.compile(keras.optimizers.SGD())\n",
    "            self.model.load_weights(weight_loc)\n",
    "        _, yhat_class = self.model.predict(testX, verbose=verbose)\n",
    "        if save_pred is not None:\n",
    "            np.save(save_pred, yhat_class)\n",
    "        if testY is not None and len(testY) == 2:\n",
    "            acc = self.model.evaluate(testX, testY, verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"The classifier and discriminator metrics for evaluation are [{}, {}]\".format(acc[0], acc[1]))\n",
    "        elif testY is not None and len(testY) == 1:\n",
    "            acc = self.model.evaluate(testX, [np.ones((testY.shape[0], 2)), testY], verbose=verbose)\n",
    "            if verbose:\n",
    "                print(\"The classifier metric for evaluation is {}\".format(acc[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "# LOAD DATASET\n",
    "print(\"Load dataset\")\n",
    "f = gzip.open('pkl/sem-relations.pkl.gz', 'rb')\n",
    "data = pkl.load(f)\n",
    "f.close()\n",
    "\n",
    "position_dims = 50\n",
    "\n",
    "embeddings = data['wordEmbeddings']\n",
    "# naming rule \n",
    "yTrain, sentenceTrain, position1Train, position2Train = data['train_set']\n",
    "yTest, sentenceTest, position1Test, position2Test = data['test_set']\n",
    "XTrain = [sentenceTrain, position1Train, position2Train]\n",
    "XTest  = [sentenceTest, position1Test, position2Test]\n",
    "\n",
    "# out of domain data, temporary use same data as above\n",
    "temp, sentenceTrainDX, position1TrainDX, position2TrainDX = data['train_set']\n",
    "temp, sentenceTestDX, position1TestDX, position2TestDX = data['test_set']\n",
    "\n",
    "DXTrain = [sentenceTrainDX, position1TrainDX, position2TrainDX]\n",
    "DXTest = [sentenceTestDX, position1TestDX, position2TestDX]\n",
    "\n",
    "max_position = max(np.max(position1Train), np.max(position2Train)) + 1\n",
    "\n",
    "n_out = max(yTrain) + 1\n",
    "\n",
    "max_sentence_len = sentenceTrain.shape[1]\n",
    "\n",
    "print(\"sentenceTrain : \", sentenceTrain.shape)\n",
    "print(\"positionTrain1 : \", position1Train.shape)\n",
    "print(\"positionTrain2 : \", position2Train.shape)\n",
    "\n",
    "print(\"sentenceTest : \", sentenceTest.shape)\n",
    "print(\"positionTest1 : \", position1Test.shape)\n",
    "print(\"positionTest2 : \", position2Test.shape)\n",
    "\n",
    "\n",
    "# Process data to match the model specs.\n",
    "print(\"333333333333333\")\n",
    "trainX = np.expand_dims(sentenceTrain, axis=2)\n",
    "trainX = np.tile(trainX, [1, 1, 2]).transpose(0, 1, 2)\n",
    "trainY = yTrain\n",
    "\n",
    "print(trainX.shape)\n",
    "\n",
    "trainX = np.expand_dims(trainX, axis=3)\n",
    "trainX = np.tile(trainX, [1, 1, 1, 1]).transpose(0, 3, 1, 2)\n",
    "print(trainX.shape)\n",
    "\n",
    "# # Divide train and test sets (the only limitation of this model is to assign even number as batch_sizes and the size of data should yield 0 remainder.)\n",
    "# trainX, testX = trainX[:45000], trainX[45000:55016]\n",
    "# trainY = keras.utils.to_categorical(trainY, num_classes=10)\n",
    "# trainY, testY = trainY[:45000], trainY[45000:55016]\n",
    "\n",
    "# Load MNIST-M dataset as out-of-domain and unlabeled data     \n",
    "trainDX = trainX[:6000]\n",
    "testDX = trainX[6000:8000]\n",
    "mnist_m = None\n",
    "\n",
    "# # Rescale -1 to 1\n",
    "# trainX = (trainX.astype(np.float32) - 127.5) / 127.5\n",
    "# trainDX = (trainDX.astype(np.float32) - 127.5) / 127.5\n",
    "# testX = (testX.astype(np.float32) - 127.5) / 127.5\n",
    "# testDX = (testDX.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Initiate the model\n",
    "dann = DANN(summary=True, width=max_sentence_len, channels=1, classes=10, features=32, batch_size=32, model_plot=True)\n",
    "# Train the model\n",
    "dann.train(trainX, trainDX, trainY, epochs=100, batch_size=32)\n",
    "# Evaluate for binary MNIST\n",
    "dann.evaluate(testX, testY, save_pred=\"./binary_testX.npy\", verbose=True)\n",
    "# Evaluate for colorful MNIST (MNIST-M)\n",
    "dann.evaluate(testDX, save_pred=\"./colorful_testX.npy\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
